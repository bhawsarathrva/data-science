{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Describe the purpose and benefits of pooling in CNN.\n",
    "Ans. Purpose of Pooling in CNNs:\n",
    "Pooling reduces the spatial dimensions (width and height) of feature maps while retaining important information. It helps in:\n",
    "\n",
    "Dimensionality Reduction: Decreasing computational load.\n",
    "\n",
    "Translation Invariance: Making the model robust to small shifts or distortions in the input.\n",
    "\n",
    "Feature Extraction: Highlighting dominant features by downsampling.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "Efficiency: Reduces memory usage and computation time.\n",
    "\n",
    "Prevents Overfitting: By summarizing features, it reduces the risk of overfitting.\n",
    "\n",
    "Improves Generalization: Focuses on important features, enhancing model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain the difference between min pooling and max pooling.\n",
    "Ans. Max Pooling:\n",
    "\n",
    "Selects the maximum value from a region of the feature map.\n",
    "\n",
    "Preserves the most prominent features, such as edges or textures.\n",
    "\n",
    "Commonly used in CNNs for its effectiveness in highlighting important features.\n",
    "\n",
    "Min Pooling:\n",
    "\n",
    "Selects the minimum value from a region of the feature map.\n",
    "\n",
    "Less commonly used, as it emphasizes the least prominent features, which may not be as useful for most tasks.\n",
    "\n",
    "Key Difference:\n",
    "\n",
    "Max pooling focuses on the strongest features, while min pooling focuses on the weakest features. Max pooling is preferred in most CNN architectures for better feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Discuss the concept of padding in CNN and its significance.\n",
    "Ans. Padding in CNN (Convolutional Neural Networks)\n",
    "Padding is the process of adding extra layers (typically zeros) around the input image before applying the convolution operation. It ensures that the output feature map maintains a desired size or shape.\n",
    "\n",
    "Types of Padding:\n",
    "Valid Padding (No Padding)\n",
    "\n",
    "No extra pixels are added.\n",
    "Output size decreases after convolution.\n",
    "where N = input size, F = filter size, S = stride.\n",
    "Same Padding (Zero Padding)\n",
    "\n",
    "Zeros are added around the input.\n",
    "Output size remains the same as the input.\n",
    "\n",
    "Ensures future information isn’t leaked in sequential data processing.\n",
    "Significance of Padding:\n",
    "✅ Preserves Spatial Dimensions – Prevents excessive shrinking of feature maps.\n",
    "✅ Better Feature Extraction – Retains edge information near borders.\n",
    "✅ Controls Overfitting – Reduces loss of image details.\n",
    "✅ Facilitates Deeper Networks – Enables deeper architectures like ResNet and VGG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compare and contrast zero-padding and valid-padding in terms of their effects on the output\n",
    "feature map size.\n",
    "Ans. Comparison of Zero-Padding and Valid-Padding in CNN\n",
    "\n",
    "Zero-Padding:\n",
    "Adds extra zeros around the input image.\n",
    "Keeps the output feature map size the same as the input (when same padding is used).\n",
    "Helps in retaining edge features by preventing shrinkage.\n",
    "Used in deeper networks to maintain spatial dimensions.\n",
    "\n",
    "Valid-Padding:\n",
    "No extra padding is added.\n",
    "Reduces the output feature map size after convolution.\n",
    "Can lead to loss of edge information as the receptive field moves.\n",
    "Used when minimizing computational cost is a priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Provide a brief overview of LeNet-5 architecture.\n",
    "Ans. LeNet-5 Architecture (1998, Yann LeCun)\n",
    "LeNet-5 is one of the earliest CNN architectures, designed for handwritten digit recognition (MNIST dataset). It consists of 7 layers (excluding input), combining convolutional and fully connected layers.\n",
    "\n",
    "Layers Overview:\n",
    "Input Layer (32×32 grayscale image)\n",
    "\n",
    "Standard MNIST images (28×28) are resized to 32×32.\n",
    "Conv Layer 1 (C1) → 6 filters (5×5) → Output: 28×28×6\n",
    "\n",
    "Extracts low-level features like edges.\n",
    "Pooling Layer 1 (S2) → Avg Pool (2×2) → Output: 14×14×6\n",
    "\n",
    "Reduces spatial size and computational complexity.\n",
    "Conv Layer 2 (C3) → 16 filters (5×5) → Output: 10×10×16\n",
    "\n",
    "Detects more complex patterns.\n",
    "Pooling Layer 2 (S4) → Avg Pool (2×2) → Output: 5×5×16\n",
    "\n",
    "Further downsampling.\n",
    "Fully Connected Layer (F5) → 120 neurons\n",
    "\n",
    "Connects convolutional layers to dense layers.\n",
    "Fully Connected Layer (F6) → 84 neurons\n",
    "\n",
    "Further feature representation.\n",
    "Output Layer → 10 neurons (Softmax for classification)\n",
    "\n",
    "Predicts digits (0-9).\n",
    "Key Features of LeNet-5:\n",
    "✅ Uses tanh activation (instead of ReLU in modern CNNs).\n",
    "✅ Avg Pooling instead of Max Pooling.\n",
    "✅ Efficient & Lightweight – Works well on low-power devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Describe the key components of LeNet-5 and their respective purposes.\n",
    "Ans. Key Components of LeNet-5 & Their Purposes\n",
    "Input Layer (32×32 grayscale image)\n",
    "\n",
    "Takes in the input image (handwritten digits).\n",
    "Standard MNIST images (28×28) are resized to 32×32 for better feature extraction.\n",
    "Convolutional Layer 1 (C1: 6 filters, 5×5, Stride 1)\n",
    "\n",
    "Extracts basic features like edges and textures.\n",
    "Produces 28×28×6 feature maps.\n",
    "Average Pooling Layer 1 (S2: 2×2, Stride 2)\n",
    "\n",
    "Reduces spatial dimensions to 14×14×6.\n",
    "Enhances feature representation while reducing computational cost.\n",
    "Convolutional Layer 2 (C3: 16 filters, 5×5, Stride 1)\n",
    "\n",
    "Detects more complex patterns from pooled features.\n",
    "Produces 10×10×16 feature maps.\n",
    "Average Pooling Layer 2 (S4: 2×2, Stride 2)\n",
    "\n",
    "Reduces size to 5×5×16.\n",
    "Helps retain important features while reducing parameters.\n",
    "Fully Connected Layer 1 (F5: 120 neurons)\n",
    "\n",
    "Flattens the feature maps and connects to dense layers.\n",
    "Extracts high-level patterns.\n",
    "Fully Connected Layer 2 (F6: 84 neurons)\n",
    "\n",
    "Further refines feature representation before classification.\n",
    "Output Layer (10 neurons, Softmax Activation)\n",
    "\n",
    "Classifies the input into one of 10 digit classes (0-9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks.\n",
    "Ans. Advantages & Limitations of LeNet-5 in Image Classification\n",
    "Advantages:\n",
    "Lightweight & Efficient – Requires fewer parameters, making it suitable for low-power devices.\n",
    "Effective for Simple Datasets – Works well on MNIST and other small-scale datasets.\n",
    "Structured Layer Design – Uses convolutional and pooling layers effectively for feature extraction.\n",
    "Pioneered CNNs – Provided the foundation for modern deep learning architectures.\n",
    "Prevents Overfitting – Uses fewer parameters compared to deeper models, reducing overfitting.\n",
    "Limitations:\n",
    "Struggles with Complex Images – Not effective for high-resolution or multi-object images.\n",
    "Shallow Architecture – Only has two convolutional layers, limiting feature extraction depth.\n",
    "Uses Tanh Activation – Slower and less efficient than ReLU, leading to vanishing gradient issues.\n",
    "Fixed Filter Size (5×5) – Lacks flexibility in handling different object sizes.\n",
    "Replaced by Deeper Models – Modern CNNs like VGG, ResNet, and EfficientNet outperform LeNet-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide\n",
    "insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:04<00:00, 2295111.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 134644.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 934122.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 821355.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Epoch 1/10, Loss: 0.2840\n",
      "Epoch 2/10, Loss: 0.0913\n",
      "Epoch 3/10, Loss: 0.0623\n",
      "Epoch 4/10, Loss: 0.0468\n",
      "Epoch 5/10, Loss: 0.0382\n",
      "Epoch 6/10, Loss: 0.0334\n",
      "Epoch 7/10, Loss: 0.0269\n",
      "Epoch 8/10, Loss: 0.0227\n",
      "Epoch 9/10, Loss: 0.0205\n",
      "Epoch 10/10, Loss: 0.0176\n",
      "Test Accuracy: 98.78%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define LeNet-5 Architecture\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)  # Output: 28x28x6\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)  # Output: 14x14x6\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)  # Output: 10x10x16\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)  # Output: 5x5x16\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)  # Output 10 classes (digits 0-9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 16*5*5)  # Flatten\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation (softmax applied in loss)\n",
    "        return x\n",
    "\n",
    "# Load MNIST Dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize Model, Loss, and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet5().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(trainloader):.4f}\")\n",
    "\n",
    "# Model Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Present an overview of the AlexNet architecture.\n",
    "Ans. Overview of AlexNet Architecture (2012)\n",
    "AlexNet is a deep CNN architecture developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. It won the ImageNet Challenge 2012 by a huge margin, demonstrating the power of deep learning for image classification.\n",
    "\n",
    "🔹 Key Features of AlexNet:\n",
    "✅ Deeper than LeNet-5 – Uses 8 layers (5 convolutional + 3 fully connected).\n",
    "✅ Uses ReLU Activation – Faster convergence compared to Tanh/Sigmoid.\n",
    "✅ Overcomes Overfitting – Uses Dropout & Data Augmentation.\n",
    "✅ Uses GPUs for Training – First major CNN to leverage parallel computation.\n",
    "\n",
    "🔹 AlexNet Architecture (Layer-wise Breakdown)\n",
    "Input Layer (224×224×3 RGB Image)\n",
    "\n",
    "Image resized from original 256×256 to 224×224.\n",
    "Conv Layer 1: 96 filters (11×11, stride 4) → Output: 55×55×96\n",
    "\n",
    "Extracts low-level features like edges and textures.\n",
    "Uses ReLU activation for faster training.\n",
    "Max Pooling 1: (3×3, stride 2) → Output: 27×27×96\n",
    "\n",
    "Reduces spatial dimensions while retaining key features.\n",
    "Conv Layer 2: 256 filters (5×5, stride 1, padding 2) → Output: 27×27×256\n",
    "\n",
    "Captures complex patterns.\n",
    "Uses ReLU activation.\n",
    "Max Pooling 2: (3×3, stride 2) → Output: 13×13×256\n",
    "\n",
    "Conv Layer 3: 384 filters (3×3, stride 1, padding 1) → Output: 13×13×384\n",
    "\n",
    "Expands feature extraction depth.\n",
    "Conv Layer 4: 384 filters (3×3, stride 1, padding 1) → Output: 13×13×384\n",
    "\n",
    "Conv Layer 5: 256 filters (3×3, stride 1, padding 1) → Output: 13×13×256\n",
    "\n",
    "Max Pooling 3: (3×3, stride 2) → Output: 6×6×256\n",
    "\n",
    "Fully Connected Layer 1: 4096 neurons + ReLU + Dropout (50%)\n",
    "\n",
    "Fully Connected Layer 2: 4096 neurons + ReLU + Dropout (50%)\n",
    "\n",
    "Output Layer: 1000 neurons (Softmax for classification)\n",
    "\n",
    "Predicts one of 1000 ImageNet classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough\n",
    "performance.\n",
    "Ans. Architectural Innovations in AlexNet\n",
    "AlexNet introduced several key innovations that significantly improved deep learning performance, leading to its breakthrough victory in the ImageNet Challenge 2012.\n",
    "\n",
    "🔹 Key Innovations & Their Impact\n",
    "Deeper Network (8 Layers)\n",
    "\n",
    "Expanded on LeNet-5 with 5 convolutional layers + 3 fully connected layers.\n",
    "Allowed extraction of complex hierarchical features.\n",
    "ReLU Activation Function\n",
    "\n",
    "Used Rectified Linear Units (ReLU) instead of Tanh/Sigmoid.\n",
    "Enabled faster training and avoided vanishing gradient issues.\n",
    "Overlapping Max Pooling\n",
    "\n",
    "Used 3×3 pooling with stride 2 instead of traditional 2×2 pooling.\n",
    "Reduced information loss while maintaining spatial hierarchy.\n",
    "Dropout Regularization\n",
    "\n",
    "Randomly dropped neurons (50%) in fully connected layers during training.\n",
    "Prevented overfitting, improving generalization.\n",
    "Use of GPUs for Training\n",
    "\n",
    "First CNN to utilize NVIDIA GPUs (two GTX 580s) for acceleration.\n",
    "Allowed training of deep networks efficiently on large datasets.\n",
    "Data Augmentation\n",
    "\n",
    "Applied image translations, reflections, and PCA-based color jittering.\n",
    "Increased dataset variability, improving robustness.\n",
    "Multiple Convolutional Kernels\n",
    "\n",
    "Used multiple-sized filters (11×11, 5×5, 3×3) in different layers.\n",
    "Enhanced feature extraction at various scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet.\n",
    "Ans. Role of Different Layers in AlexNet\n",
    "1️. Convolutional Layers (Feature Extraction)\n",
    "Purpose: Extract low-level to high-level features (edges, textures, objects).\n",
    "AlexNet Uses: 5 Conv Layers with different filter sizes (11×11, 5×5, 3×3).\n",
    "Effect: Helps in hierarchical feature learning for image classification.\n",
    "2️. Pooling Layers (Dimensionality Reduction)\n",
    "Purpose: Reduce spatial size while retaining key features.\n",
    "AlexNet Uses: Overlapping Max Pooling (3×3, stride 2) after Conv layers.\n",
    "Effect: Reduces computational cost, prevents overfitting, and improves translation invariance.\n",
    "3️. Fully Connected Layers (Classification & Decision Making)\n",
    "Purpose: Flatten extracted features and classify images.\n",
    "AlexNet Uses: 3 Fully Connected Layers (4096, 4096, 1000 neurons).\n",
    "Effect: Maps learned features to ImageNet's 1000 classes with softmax activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Implement AlexNet using a deep learning framework of your choice and evaluate its performance\n",
    "on a dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define AlexNet Architecture\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize Model, Loss, and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AlexNet(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(trainloader):.4f}\")\n",
    "\n",
    "# Model Evaluation\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
