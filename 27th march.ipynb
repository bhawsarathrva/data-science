{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "Ans-R-squared, or the coefficient of determination, measures how well the regression line fits the data points. It represents the proportion of variance in the dependent variable that is predictable from the independent variables. \n",
    "formula-\n",
    "R-squared = 1 - (residual sum of squares / total sum of squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "Answer:\n",
    "Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in the model. Unlike R-squared, which can increase with additional predictors even if they don't improve the model, adjusted R-squared penalizes the addition of non-significant predictors.\n",
    "formula- \n",
    "![alt text](<Screenshot 2024-11-12 010306.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "Ans:\n",
    "Adjusted R-squared is more appropriate when comparing models with different numbers of predictors. It is especially useful for multiple regression models to avoid overfitting, as it only increases if the new predictor improves the model more than expected by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "Answer:\n",
    "\n",
    "Mean Squared Error (MSE): Represents the average of the squared differences between actual and predicted values. Calculated as:\n",
    "\n",
    "![alt text](<Screenshot 2024-11-12 010503.png>)\n",
    "\n",
    "Root Mean Squared Error (RMSE): The square root of MSE, it represents the standard deviation of the residuals, giving a measure in the same units as the original data:\n",
    "\n",
    "![alt text](<Screenshot 2024-11-12 010532.png>)\n",
    "\n",
    "Mean Absolute Error (MAE): The average of the absolute differences between actual and predicted values, calculated as:\n",
    "\n",
    "![alt text](<Screenshot 2024-11-12 010540.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "Ans:\n",
    "\n",
    "RMSE: Advantageous when large errors are particularly undesirable, but sensitive to outliers.\n",
    "MSE: Similar to RMSE but harder to interpret in terms of original units. Good for optimizations in model training but also sensitive to outliers.\n",
    "MAE: More robust to outliers and easier to interpret. However, it doesn’t emphasize large errors as much as RMSE/MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "Ans:\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) adds a penalty equal to the absolute value of the coefficients. It can zero out coefficients, effectively selecting features. Ridge regularization (L2 penalty) shrinks coefficients towards zero without setting any to zero. Lasso is appropriate when feature selection is desired, while Ridge is better when all features are relevant but require regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "Ans:\n",
    "Regularization adds a penalty to large coefficients, discouraging complex models that fit noise in the data. For example, in a polynomial regression with high degrees, regularization can help keep coefficients small, thus preventing overfitting by smoothing the model’s fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "Ans:\n",
    "Regularized models may oversimplify if relevant features have coefficients that are shrunk too much. They also struggle when relationships are nonlinear or feature interactions are significant, as they don’t inherently capture complex patterns beyond linear associations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "Ans:\n",
    "MAE suggests that Model B has a lower average error than Model A. However, since RMSE emphasizes larger errors, Model A might perform better in scenarios with few outliers. Choosing based solely on one metric might not provide a complete picture, as the context of the application should guide the choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
    "Ans:\n",
    "The choice depends on feature importance. If feature selection is critical, Lasso (Model B) may be preferred. If all features contribute value without needing to zero out any, Ridge (Model A) would be better. However, Lasso might discard relevant features, and Ridge may retain irrelevant ones. Both methods have trade-offs based on data sparsity and feature relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
