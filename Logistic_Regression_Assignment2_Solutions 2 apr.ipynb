{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b40214",
   "metadata": {},
   "source": [
    "\n",
    "### Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n",
    "**Grid Search CV** (Cross-Validation) is used to tune hyperparameters of a model by systematically searching through a specified parameter grid. \n",
    "It evaluates model performance for each combination of parameters using cross-validation and selects the best combination based on a scoring metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241aacc",
   "metadata": {},
   "source": [
    "\n",
    "### Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?\n",
    "- **Grid Search CV** exhaustively searches all possible combinations of hyperparameters.\n",
    "- **Randomized Search CV** samples a fixed number of hyperparameter combinations randomly.\n",
    "\n",
    "**Use Grid Search** when you have fewer hyperparameters or a smaller search space. **Use Randomized Search** when dealing with large search spaces or computational constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7315eb0a",
   "metadata": {},
   "source": [
    "\n",
    "### Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "**Data leakage** occurs when information from outside the training dataset is used to create the model, leading to overly optimistic performance metrics.  \n",
    "**Example**: Using future data or target variable information in the training set can cause data leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6435f",
   "metadata": {},
   "source": [
    "\n",
    "### Q4. How can you prevent data leakage when building a machine learning model?\n",
    "- Ensure proper train-test splits.\n",
    "- Avoid using target variable information in feature engineering.\n",
    "- Apply data transformations (e.g., scaling, encoding) within cross-validation folds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2100371",
   "metadata": {},
   "source": [
    "\n",
    "### Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "A **confusion matrix** is a table that summarizes the performance of a classification model by comparing predicted and actual class labels.  \n",
    "It provides counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a87f1",
   "metadata": {},
   "source": [
    "\n",
    "### Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n",
    "- **Precision**: The proportion of true positive predictions among all positive predictions (TP / (TP + FP)).\n",
    "- **Recall**: The proportion of true positive predictions among all actual positive cases (TP / (TP + FN)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe28a2e",
   "metadata": {},
   "source": [
    "\n",
    "### Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n",
    "Analyze the **false positives (FP)** and **false negatives (FN)**:\n",
    "- **FP** indicates over-prediction of the positive class.\n",
    "- **FN** indicates under-prediction of the positive class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ce024",
   "metadata": {},
   "source": [
    "\n",
    "### Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?\n",
    "- **Accuracy**: (TP + TN) / (TP + TN + FP + FN)\n",
    "- **Precision**: TP / (TP + FP)\n",
    "- **Recall**: TP / (TP + FN)\n",
    "- **F1 Score**: 2 * (Precision * Recall) / (Precision + Recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b28b28c",
   "metadata": {},
   "source": [
    "\n",
    "### Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
    "**Accuracy** depends on the ratio of correct predictions (TP + TN) to total predictions. However, it can be misleading if the dataset is imbalanced, emphasizing the need for precision, recall, or F1-score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b1864",
   "metadata": {},
   "source": [
    "\n",
    "### Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?\n",
    "By analyzing the distribution of errors (FP and FN), you can detect biases toward specific classes or limitations in handling certain data patterns. Class imbalance can also be identified and addressed.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
