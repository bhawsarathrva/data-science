{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an \n",
    "example of each.\n",
    "\n",
    "Ans. \n",
    "Simple Linear Regression:\n",
    "    Simple linear regression models the association     between a dependent variable(also known as the     response variable) and asingle independent variable     (predictor). The connection is considered to be     linear, whichmeans it may be represented with a     straightline.\n",
    "Ex:\n",
    "Suppose we want to predict a person's weight (in kilograms) based on their height (in centimeters). Here, weight is the dependent variable, and height is the independent variable.\n",
    "\n",
    "Dependent variable (y): Weight\n",
    "Independent variable (x): Height\n",
    "\n",
    "Weight=Bnot+B1*Height+e\n",
    "\n",
    "Multiple Linear Regression:\n",
    "Multiple linear regression models the connection between a dependent variable and two or more independent variables. It expands on the notion of basic linear regression by using several predictors to account for fluctuations in the dependant variable.\n",
    "\n",
    "Ex:Suppose we want to predict a person's weight based not only on their height but also on their age and gender. Here, weight is still the dependent variable, but now we have three independent variables: height, age, and gender.\n",
    "\n",
    "Dependent variable (y): Weight\n",
    "Independent variables (x_1, x_2, x_3): Height, Age, Gender\n",
    "The multiple linear regression model might look like:\n",
    "Weight=Bnot+B1*Height+B2*Age+B3*Gender+e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in \n",
    "a given dataset?\n",
    "Ans. The assumptions of linear regression are:\n",
    "1. Linearity:The independent variables (predictors) have a linear connection with the dependent variable.\n",
    "2. Independence of errors:The residuals (errors) are independent of one another. In other words, there should be no association among the residuals.\n",
    "3. Homoscedasticity:The residuals have a consistent variance at all levels of the independent variables. This indicates that the residuals should be relatively equal across all anticipated values.\n",
    "4. Normality of Residuals:\n",
    "5. No Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using \n",
    "a real-world scenario.\n",
    "Ans.The slope and intercept in a linear regression model have intuitive interpretations based on the connection between the independent (predictor) variable(s) and the dependent (outcome) variable.\n",
    "\n",
    "\n",
    "Intercept: The intercept indicates the dependent variable's expected value when all of the independent variables are 0. It's the point where the regression line intersects the y-axis.\n",
    "In many practical scenarios, the intercept may not necessarily have a relevant real-world interpretation, especially if zero lies outside the data range.\n",
    "\n",
    "Slope:The slope shows the change in the dependent variable for every one-unit rise in the independent variable, provided all other variables remain constant. It measures the degree and direction of the association between the predictor and the result.\n",
    "If the slope is positive, it shows that as the independent variable grows, so does the dependent variable. A negative slope shows a negative relationship: as the independent variable grows, the dependent variable declines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Ans. Gradient Descent is an optimisation procedure that minimises the cost (or loss) function in machine learning models, particularly those with numerous parameters. The purpose is to repeatedly alter the parameters (or weights) to identify the set that minimises prediction error.\n",
    "\n",
    "Working of GD:\n",
    "1.  Initialize the parameters (weights) of the model.\n",
    "2.  Calculate the gradient of the cost function with respect to the parameters.\n",
    "3.  Update the parameters by subtracting the product of the learning rate and the gradient\n",
    "4.  Repeat steps 2 and 3 until convergence or a stopping criterion is met.\n",
    "5.  The process is repeated until the model's performance on the training data is maximised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "Ans. Multiple Linear Regression expands the notion of basic linear regression by modelling the connection between one dependent variable and two or more independent (predictor) variables. The model takes the following general form:\n",
    "\n",
    "ùë¶ = ùõΩ 0 + ùõΩ 1 ùë• 1 + ùõΩ 2 ùë• 2 + .. + ùõΩ ùëõ ùë• ùëõ +ùúñ\n",
    "Difference from Simple Linear Regression:\n",
    "1. Number of prediction\n",
    "2. Equation\n",
    "3. Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and \n",
    "address this issue?\n",
    "\n",
    "Ans. Multicollinearity arises when two or more independent variables in a multiple linear regression model are strongly linked, resulting in duplicate or overlapping information. This makes it impossible to separate each predictor's unique influence on the dependent variable, resulting in unstable regression coefficient estimations.\n",
    "\n",
    "Detecting Multicollinearity:\n",
    "Variance Inflation Factor:\n",
    "\n",
    "VIF assesses the extent to which a coefficient's variance is exaggerated by multicollinearity. A VIF greater than 5 or 10 indicates a significant level of multicollinearity.\n",
    "Correlation Matrix:\n",
    "\n",
    "Examining the correlation between independent variables might reveal pairings that are strongly associated (for example, a correlation greater than 0.8).\n",
    "Eigenvalues:\n",
    "\n",
    "Small eigenvalues in the correlation matrix suggest multicollinearity.\n",
    "\n",
    "Addressing Multicollinearity:\n",
    "Remove highly correlated predictors:\n",
    "\n",
    "Remove one or more strongly linked variables from the model.\n",
    "Combine predictors:\n",
    "\n",
    "Combine connected variables into a single composite variable, such as via Principal Component Analysis (PCA).\n",
    "Regularisation Techniques:\n",
    "\n",
    "on limit the impact of multicollinearity, use approaches such as Ridge regression or Lasso, which apply penalties on coefficient sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "Ans. Polynomial Regression Model.\n",
    "Polynomial regression is a sort of regression analysis in which the connection between the dependent and independent variables is represented by an nth-degree polynomial. The model can capture nonlinear interactions by incorporating predictors with higher degrees (squared, cubed, etc.).\n",
    "\n",
    "Difference from Linear Regression:\n",
    "1. Nature of Relationship\n",
    "2. Model Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear \n",
    "regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "Ans. Advantages of Polynomial Regression Compared to Linear Regression\n",
    "Captures Nonlinear Relationships:\n",
    "\n",
    "Polynomial regression can model complex, nonlinear relationships between the dependent and independent variables, whereas linear regression is limited to straight-line relationships.\n",
    "Flexibility:\n",
    "\n",
    "By increasing the degree of the polynomial, polynomial regression can fit data more accurately, offering greater flexibility in modeling various data patterns.\n",
    "Disadvantages of Polynomial Regression\n",
    "Overfitting:\n",
    "\n",
    "Higher-degree polynomials can lead to overfitting, where the model fits the noise in the data rather than the true underlying relationship. This reduces the model‚Äôs generalizability to new data.\n",
    "Interpretability:\n",
    "\n",
    "The model becomes harder to interpret as the degree of the polynomial increases, making it difficult to explain how each term affects the outcome.\n",
    "Extrapolation Issues:\n",
    "\n",
    "Polynomial regression models can behave erratically outside the range of the data, leading to unrealistic predictions when extrapolating.\n",
    "Increased Complexity:\n",
    "\n",
    "Adding polynomial terms increases the model's complexity and can introduce multicollinearity among the higher-order terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
