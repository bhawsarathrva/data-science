{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "Ans. he decision boundary of a linear SVM can be represented as:\n",
    "      w 路 x + b = 0\n",
    " where w is the weight vector, x is the input feature vector, and b is the bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the objective function of a linear SVM?\n",
    "Ans. The linear SVM aims to minimize the following objective:\n",
    "     1/2 ||w||^2\n",
    " Subject to the constraint:\n",
    "     y_i(w 路 x_i + b) >= 1 for all i,\n",
    " where y_i represents the class labels (-1 or +1), and x_i represents the input samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is the kernel trick in SVM?\n",
    "Ans.  The kernel trick allows SVMs to perform classification tasks in higher-dimensional spaces without explicitly computing the transformation of data points. \n",
    " Instead of mapping the data to a higher-dimensional space, the kernel function computes the dot product in the feature space directly.\n",
    " Popular kernel functions include:\n",
    " - Linear kernel: K(x, y) = x 路 y\n",
    " - Polynomial kernel: K(x, y) = (x 路 y + c)^d\n",
    " - Radial Basis Function (RBF) kernel: K(x, y) = exp(-||x - y||^2 / (2 * sigma^2))\n",
    " The kernel trick is particularly useful for non-linear data that is not separable in its original space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "Ans.\n",
    "Support vectors are the critical data points that lie closest to the decision boundary (or hyperplane).\n",
    "These points determine the position and orientation of the hyperplane, as the SVM algorithm maximizes the margin between the support vectors of the two classes.\n",
    "Any changes to these support vectors will impact the decision boundary, making them vital to the model's performance.\n",
    "Example:\n",
    "In the Hard Margin SVM visualized above:\n",
    "\n",
    "Support vectors are highlighted as circled points on the margin boundaries (dotted lines).\n",
    "These points lie either directly on or within the margins of the separating hyperplane.\n",
    "In the Soft Margin SVM:\n",
    "\n",
    "Support vectors can lie within the margin or even on the wrong side of the hyperplane, depending on the flexibility provided by the slack variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_hard = SVC(kernel='linear', C=1e10)\n",
    "svm_hard.fit(X_train, y_train)\n",
    "\n",
    "# Plot decision boundary and support vectors\n",
    "plt.figure(figsize=(8, 6))\n",
    "def plot_svm(model, X, y):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=50, edgecolors='k')\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # Create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    Z = model.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "    # Plot decision boundary and margins\n",
    "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.7,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    # Plot support vectors\n",
    "    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "               s=100, linewidth=1, facecolors='none', edgecolors='k')\n",
    "\n",
    "plot_svm(svm_hard, X_train, y_train)\n",
    "plt.title(\"Hard Margin SVM\")\n",
    "plt.show()\n",
    "\n",
    "# Training a Linear SVM with Soft Margin\n",
    "svm_soft = SVC(kernel='linear', C=0.1)\n",
    "svm_soft.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_svm(svm_soft, X_train, y_train)\n",
    "plt.title(\"Soft Margin SVM\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. SVM Implementation through Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear SVM using scikit-learn\n",
    "svm_model = SVC(kernel='linear', C=1)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Decision Boundary for scikit-learn SVM\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_svm(svm_model, X_test, y_test)\n",
    "plt.title(\"Decision Boundary (scikit-learn SVM)\")\n",
    "plt.show()\n",
    "\n",
    "### Bonus Task: Linear SVM Implementation from Scratch\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
    "                    self.b -= self.learning_rate * y_[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return np.sign(approx)\n",
    "\n",
    "# Train Linear SVM from Scratch\n",
    "custom_svm = LinearSVM()\n",
    "custom_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_custom = custom_svm.predict(X_test)\n",
    "accuracy_custom = accuracy_score(y_test, np.where(y_pred_custom > 0, 1, 0))\n",
    "print(f\"Accuracy (Custom SVM): {accuracy_custom:.2f}\")\n",
    "\n",
    "### Comparison\n",
    "print(f\"Accuracy Comparison: scikit-learn SVM = {accuracy:.2f}, Custom SVM = {accuracy_custom:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
